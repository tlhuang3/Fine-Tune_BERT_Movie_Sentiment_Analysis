{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for fine-tuning BERT for sentiment analysis on the movie reviews dataset.\n",
    "### In this example we use BCE loss. But the loss function can be changed to other loss functions, such as triplet loss, contrastive loss, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description    y\n",
      "0  For a movie that gets no respect there sure ar...  1.0\n",
      "1  Bizarre horror movie filled with famous faces ...  1.0\n",
      "2  A solid, if unremarkable film. Matthau, as Ein...  1.0\n",
      "3  It's a strange feeling to sit alone in a theat...  1.0\n",
      "4  You probably all already know this by now, but...  1.0\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"./all_train.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['description'].tolist(), df['y'].tolist()\n",
    "\n",
    "# Tokenize and encode the data\n",
    "def encode_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels, dtype=torch.float))\n",
    "\n",
    "# Fine-tune BERT\n",
    "def fine_tune_bert(train_dataset, val_dataset, batch_size=16, epochs=3, learning_rate=2e-5, patience=2):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = loss_fn(outputs.logits.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs.logits.squeeze() > 0).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, best_val_loss, best_val_accuracy, epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "texts, labels = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data\n",
    "dataset = encode_data(texts, labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test\n",
    "train_val_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_dataset, val_dataset = train_test_split(train_val_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the BERT model with different hyperparameters and save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning with batch_size=8, epochs=3, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 4000/4000 [02:30<00:00, 26.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3106\n",
      "Validation Loss: 0.2835, Accuracy: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 4000/4000 [02:28<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1575\n",
      "Validation Loss: 0.2786, Accuracy: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 4000/4000 [02:28<00:00, 26.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0579\n",
      "Validation Loss: 0.3752, Accuracy: 0.8984\n",
      "Final Validation Loss: 0.2786, Accuracy: 0.8966\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=8, epochs=3, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 4000/4000 [02:29<00:00, 26.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3379\n",
      "Validation Loss: 0.2732, Accuracy: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 4000/4000 [02:29<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1702\n",
      "Validation Loss: 0.2930, Accuracy: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 4000/4000 [02:29<00:00, 26.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0467\n",
      "Validation Loss: 0.3838, Accuracy: 0.8909\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2732, Accuracy: 0.8856\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=8, epochs=4, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 4000/4000 [02:30<00:00, 26.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3134\n",
      "Validation Loss: 0.2602, Accuracy: 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 4000/4000 [02:29<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1654\n",
      "Validation Loss: 0.2857, Accuracy: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 4000/4000 [02:29<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0647\n",
      "Validation Loss: 0.3580, Accuracy: 0.8954\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2602, Accuracy: 0.8912\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=8, epochs=4, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 4000/4000 [02:30<00:00, 26.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3448\n",
      "Validation Loss: 0.3009, Accuracy: 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 4000/4000 [02:29<00:00, 26.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1854\n",
      "Validation Loss: 0.3084, Accuracy: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 4000/4000 [02:29<00:00, 26.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0681\n",
      "Validation Loss: 0.3643, Accuracy: 0.8904\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.3009, Accuracy: 0.8719\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=8, epochs=5, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 4000/4000 [02:29<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3171\n",
      "Validation Loss: 0.2655, Accuracy: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4000/4000 [02:28<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1682\n",
      "Validation Loss: 0.2674, Accuracy: 0.8945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4000/4000 [02:28<00:00, 26.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0695\n",
      "Validation Loss: 0.3985, Accuracy: 0.8898\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2655, Accuracy: 0.8889\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=8, epochs=5, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 4000/4000 [02:28<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3445\n",
      "Validation Loss: 0.3009, Accuracy: 0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4000/4000 [02:29<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1895\n",
      "Validation Loss: 0.2882, Accuracy: 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4000/4000 [02:28<00:00, 27.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0789\n",
      "Validation Loss: 0.3420, Accuracy: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 4000/4000 [02:28<00:00, 27.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0277\n",
      "Validation Loss: 0.4439, Accuracy: 0.8971\n",
      "Early stopping triggered after epoch 4\n",
      "Final Validation Loss: 0.2882, Accuracy: 0.8892\n",
      "Epochs run: 4\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=3, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3085\n",
      "Validation Loss: 0.2819, Accuracy: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1657\n",
      "Validation Loss: 0.2705, Accuracy: 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0746\n",
      "Validation Loss: 0.3543, Accuracy: 0.8929\n",
      "Final Validation Loss: 0.2705, Accuracy: 0.8928\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=3, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3200\n",
      "Validation Loss: 0.2656, Accuracy: 0.8915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1560\n",
      "Validation Loss: 0.2710, Accuracy: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2000/2000 [01:39<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0440\n",
      "Validation Loss: 0.3988, Accuracy: 0.8961\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2656, Accuracy: 0.8915\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=4, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 2000/2000 [01:38<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3070\n",
      "Validation Loss: 0.2800, Accuracy: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 2000/2000 [01:39<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1691\n",
      "Validation Loss: 0.2670, Accuracy: 0.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 2000/2000 [01:39<00:00, 20.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0720\n",
      "Validation Loss: 0.3709, Accuracy: 0.8908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 2000/2000 [01:38<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.0303\n",
      "Validation Loss: 0.4472, Accuracy: 0.8958\n",
      "Early stopping triggered after epoch 4\n",
      "Final Validation Loss: 0.2670, Accuracy: 0.8946\n",
      "Epochs run: 4\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=4, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 2000/2000 [01:39<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3193\n",
      "Validation Loss: 0.2786, Accuracy: 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 2000/2000 [01:39<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1628\n",
      "Validation Loss: 0.2797, Accuracy: 0.8981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 2000/2000 [01:39<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0529\n",
      "Validation Loss: 0.4053, Accuracy: 0.8966\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2786, Accuracy: 0.8906\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=5, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 2000/2000 [01:39<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3076\n",
      "Validation Loss: 0.2673, Accuracy: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 2000/2000 [01:38<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1686\n",
      "Validation Loss: 0.2713, Accuracy: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 2000/2000 [01:38<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0786\n",
      "Validation Loss: 0.3130, Accuracy: 0.8976\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2673, Accuracy: 0.8879\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=16, epochs=5, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 2000/2000 [01:39<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3446\n",
      "Validation Loss: 0.2903, Accuracy: 0.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 2000/2000 [01:39<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1836\n",
      "Validation Loss: 0.2800, Accuracy: 0.8909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 2000/2000 [01:39<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0772\n",
      "Validation Loss: 0.3700, Accuracy: 0.8891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 2000/2000 [01:39<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0297\n",
      "Validation Loss: 0.4538, Accuracy: 0.8956\n",
      "Early stopping triggered after epoch 4\n",
      "Final Validation Loss: 0.2800, Accuracy: 0.8909\n",
      "Epochs run: 4\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=3, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 1000/1000 [01:26<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3123\n",
      "Validation Loss: 0.2727, Accuracy: 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [01:25<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1785\n",
      "Validation Loss: 0.2882, Accuracy: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [01:26<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0974\n",
      "Validation Loss: 0.3276, Accuracy: 0.8956\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2727, Accuracy: 0.8864\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=3, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 1000/1000 [01:26<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.3070\n",
      "Validation Loss: 0.2672, Accuracy: 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [01:26<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1520\n",
      "Validation Loss: 0.2737, Accuracy: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [01:26<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0481\n",
      "Validation Loss: 0.3822, Accuracy: 0.9005\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2672, Accuracy: 0.8912\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=4, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3130\n",
      "Validation Loss: 0.2703, Accuracy: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1832\n",
      "Validation Loss: 0.2847, Accuracy: 0.8944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0959\n",
      "Validation Loss: 0.3429, Accuracy: 0.8940\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2703, Accuracy: 0.8899\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=4, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 0.3160\n",
      "Validation Loss: 0.2740, Accuracy: 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 1000/1000 [01:25<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Loss: 0.1614\n",
      "Validation Loss: 0.2687, Accuracy: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Loss: 0.0583\n",
      "Validation Loss: 0.3415, Accuracy: 0.8928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 1000/1000 [01:26<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Loss: 0.0186\n",
      "Validation Loss: 0.4587, Accuracy: 0.8956\n",
      "Early stopping triggered after epoch 4\n",
      "Final Validation Loss: 0.2687, Accuracy: 0.8950\n",
      "Epochs run: 4\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=5, learning_rate=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 1000/1000 [01:25<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3153\n",
      "Validation Loss: 0.2667, Accuracy: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1000/1000 [01:26<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1844\n",
      "Validation Loss: 0.2701, Accuracy: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1000/1000 [01:26<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0976\n",
      "Validation Loss: 0.3366, Accuracy: 0.8964\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2667, Accuracy: 0.8875\n",
      "Epochs run: 3\n",
      "\n",
      "Fine-tuning with batch_size=32, epochs=5, learning_rate=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5: 100%|██████████| 1000/1000 [01:26<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3209\n",
      "Validation Loss: 0.2721, Accuracy: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1000/1000 [01:25<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1657\n",
      "Validation Loss: 0.2997, Accuracy: 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1000/1000 [01:26<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0637\n",
      "Validation Loss: 0.3514, Accuracy: 0.8941\n",
      "Early stopping triggered after epoch 3\n",
      "Final Validation Loss: 0.2721, Accuracy: 0.8904\n",
      "Epochs run: 3\n",
      "\n",
      "Best parameters: {'batch_size': 8, 'epochs': 3, 'learning_rate': 2e-05}\n",
      "Best validation accuracy: 0.8966\n",
      "Test accuracy with best model: 0.8973\n",
      "Fine-tuning complete. Best model saved.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters to experiment with\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs_list = [3, 4, 5]\n",
    "learning_rates = [2e-5, 5e-5]\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_params = {}\n",
    "best_overall_model = None\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for epochs in epochs_list:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nFine-tuning with batch_size={batch_size}, epochs={epochs}, learning_rate={lr}\")\n",
    "            model, val_loss, val_accuracy, epochs_run = fine_tune_bert(train_dataset, val_dataset, batch_size=batch_size, epochs=epochs, learning_rate=lr)\n",
    "            \n",
    "            print(f\"Final Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Epochs run: {epochs_run}\")\n",
    "            \n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_params = {'batch_size': batch_size, 'epochs': epochs_run, 'learning_rate': lr}\n",
    "                best_overall_model = model\n",
    "            \n",
    "            # Clear CUDA cache to free up memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Final evaluation on the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "best_overall_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(best_overall_model.device) for b in batch]\n",
    "        outputs = best_overall_model(input_ids, attention_mask=attention_mask)\n",
    "        predicted = (outputs.logits.squeeze() > 0).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Test accuracy with best model: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_overall_model.save_pretrained('best_fine_tuned_bert_sentiment')\n",
    "tokenizer.save_pretrained('best_fine_tuned_bert_sentiment')\n",
    "print(\"Fine-tuning complete. Best model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the sentiments for some example reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie was absolutely fantastic! I loved every minute of it.\n",
      "Sentiment: Positive\n",
      "Confidence: 0.9910\n",
      "\n",
      "Text: The film was a complete disaster. Terrible acting and a nonsensical plot.\n",
      "Sentiment: Negative\n",
      "Confidence: 0.9982\n",
      "\n",
      "Text: It was an okay movie. Nothing special, but not terrible either.\n",
      "Sentiment: Positive\n",
      "Confidence: 0.5151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def load_model(model_path):\n",
    "    # Load the fine-tuned model\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    # Load the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Apply sigmoid to get probability\n",
    "    probs = F.sigmoid(logits)\n",
    "    \n",
    "    # Get the predicted class (0 for negative, 1 for positive)\n",
    "    predicted_class = (probs > 0.5).int().item()\n",
    "    \n",
    "    # Get the probability of the predicted class\n",
    "    confidence = probs.item() if predicted_class == 1 else 1 - probs.item()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Load the model\n",
    "model_path = 'best_fine_tuned_bert_sentiment'\n",
    "model, tokenizer = load_model(model_path)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"The film was a complete disaster. Terrible acting and a nonsensical plot.\",\n",
    "    \"It was an okay movie. Nothing special, but not terrible either.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    sentiment, confidence = predict_sentiment(text, model, tokenizer)\n",
    "    sentiment_label = \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment_label}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaizhang_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
